\documentclass[12]{article}

\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{color}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{hyperref}

\definecolor{lightgray}{gray}{0.5}
\pagestyle{fancy}

\title{CISC 360 Project}
\author{Ryan McKenna, Matthew Paul, Niko Gerassimakis,\\ Neil Duffy, James Kerrigan}

\lstset{language=C, frame=single, showstringspaces=false}

\begin{document}
\maketitle

\section{Introduction}

\subsection{Background}

LU Factorization is the most common technique used to solve systems of linear equations.  It is most useful when solving a dense linear system and is only appropriate when the system is square.  It works by decomposing a square matrix $A$ into an lower triangle matrix, $L$ and an upper triangular matrix $U$ such that 

$$ A = LU $$

\subsection{Impact}

Triangular matrices have a number of nice properties that make them easier to work with.  For example, if $ T $ is a triangular matrix, then you can solve the equation

$$ T x = b $$ 

in $O(n^2)$ time, as opposed to $O(n^3)$ for full matrices.  Computationally, this means we can solve $k$ equations of the form 

$$ A x = b_i $$

for $ 1 \leq i \leq k $ in $ O(n^3 + k n^2) $ as opposed to $ O(k n^3) $.

\subsection{Use Cases}

Dense linear algebra is very important for mathematicians, scientists, and engineers alike.  Linear algebra comes up in so many situations:

\begin{itemize}
\item Physics
\item Partial differential equations
\item Graph theory
\item Statistics / Curve Fitting
\item Sports Ranking
\end{itemize}

Solving linear systems is a key component of linear algebra, and LU factorization is the best known way to solve these systems.  Having a highly optimized LU Factorization algorithm gives you the ability to (1) solve systems faster, and (2) solve bigger systems. 

\section{Existing Work}

\section{Approach}

We will start off with a simple implementation of LU factorization that is not optimized for any particular architecture which will serve as a starting point to measure performance improvements.  This implementation will be a direct translation of the algorithm described in this video: 
\url{https://www.youtube.com/watch?v=UlWcofkUDDU}.  One nice property that this code has is that it's 'obviously correct', so it's very readable.  That being said, it is completely oblivious to the cache configuration and the underlying architecture that it is intended to be executed on.

\subsection{Architecture}

Our initial optimizations target a multicore CPU architecture, such as the current Intel Haswell series i7 4970K processor. By utilizing OpenMP, our code will be accessible on a wide variety of platforms. This project may also utilize CUDA in order to implement parallel optimizations on the GPU. GPU parallelization is advantageous as it allows us to take advantage of a large number of cores + threads. Code optimizations will target the most recent nvidia GTX 970 GPU. 

\subsection{Optimizations}

TO-DO: cache, loop unrolling, openMP

\section{Results}

\subsection{Sequential Optimizations}

\subsection{Parallel Optimizations}

\section{Conclusion}

\end{document}